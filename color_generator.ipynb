{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new colors based on text\n",
    "* Enter some text and the model outputs a new color block \n",
    "* Trained on existing color names and color RBG values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmO1WEDPp1xy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3sd_AEJspQJg",
    "outputId": "4f4e9640-518f-455f-afe4-10f4ead6b64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,red,green,blue\r\n",
      "parakeet,174,182,87\r\n",
      "saddle brown,88,52,1\r\n",
      "cucumber crush,222,237,215\r\n",
      "pool blue,134,194,201\r\n",
      "distance,98,110,130\r\n",
      "light urple,179,111,246\r\n",
      "east side,172,145,206\r\n",
      "florida seashells,250,228,199\r\n",
      "paris,145,167,189\r\n"
     ]
    }
   ],
   "source": [
    "# Download the colors dataset\n",
    "if not os.path.exists('colors.csv'):\n",
    "  !curl -O 'https://raw.githubusercontent.com/random-forests/datasets/master/colors.csv'\n",
    "!head colors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mG0WnNZtmxul",
    "outputId": "890be797-1779-47de-8a5c-795b0a50fc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14157 colors downloaded\n",
      "For example ('parakeet', 174, 182, 87)\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "colors_rgb = []\n",
    "csv_reader = csv.reader(open('colors.csv'), delimiter=',')\n",
    "next(csv_reader) # Remove the header\n",
    "for row in csv_reader:\n",
    "    name, r, g, b = row[0].lower().strip(), int(row[1]), int(row[2]), int(row[3])\n",
    "    colors_rgb.append((name, r, g, b))\n",
    "print(len(colors_rgb), 'colors downloaded')\n",
    "print('For example', colors_rgb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PdIWQdlBrZXA",
    "outputId": "c2403fdf-6386-44a7-d359-b5c9b771e09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parakeet 174 182 87\n"
     ]
    }
   ],
   "source": [
    "# In this experiment, we will train a char-baed RNN to generate a line of text\n",
    "# that resembles this dataset (we'll treat each line as a string)\n",
    "sentences = []\n",
    "for row in colors_rgb:\n",
    "  line = ' '.join([str(part) for part in row])\n",
    "  sentences.append(line)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Q_o2oIUNuyWZ",
    "outputId": "c32330a8-ebec-43ec-f56b-2d8a672f4a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 38\n",
      "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for our char-based RNN\n",
    "chars = set()\n",
    "for sentence in sentences:\n",
    "  for char in sentence:\n",
    "    chars.add(char)\n",
    "    \n",
    "# add a special char for padding\n",
    "chars.add('<pad>')\n",
    "\n",
    "vocab = sorted(set(chars))\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "char2idx = {u : i for i, u in enumerate(vocab)}\n",
    "idx2char = {i : u for i, u in enumerate(vocab)}\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(vocab)\n",
    "print('vocab size:', vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p5sdKRJ6rp8I",
    "outputId": "6e61bce6-7584-4a65-c30f-f757cbd5b880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized sentence [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text\n",
    "text_int = []\n",
    "for sentence in sentences:\n",
    "  int_sentence = [] \n",
    "  for c in sentence:\n",
    "    int_sentence.append(char2idx[c])\n",
    "  text_int.append(int_sentence)\n",
    "print('Vectorized sentence', text_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "0g6Do15As4yQ",
    "outputId": "270e6be0-07c4-4381-bc17-e5b782f2b45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "# pad sentences to max_length\n",
    "max_length = 40\n",
    "for sentence in text_int:\n",
    "  while (len(sentence) < max_length):\n",
    "    sentence.append(char2idx['<pad>'])\n",
    "print('Padded sentences', text_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-y87WF_Fw1kJ",
    "outputId": "f3886555-8175-43d5-c28d-77c680ccf8d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "# truncate all sentences to max_length\n",
    "for i in range(len(text_int)):\n",
    "  sentence = text_int[i]\n",
    "  if len(sentence) > max_length:\n",
    "    text_int[i] = sentence[:max_length]\n",
    "print(\"Truncated sentences\", text_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "mwu3FgSpxNWT",
    "outputId": "aefdfe0e-74ea-4bd6-9d55-8fdc88ef25e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example, target\n",
      "[27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "[12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for i in range(len(text_int)):\n",
    "  inps = text_int[i][:max_length-1]\n",
    "  targ = text_int[i][1:max_length]\n",
    "  input_text.append(inps)\n",
    "  target_text.append(targ)\n",
    "  \n",
    "print(\"First training example, target\")  \n",
    "print(input_text[0])\n",
    "print(target_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjyeDrvGzl8E"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT8ed7cu0w-z"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "   \n",
    "    self.rnn = tf.keras.layers.GRU(self.units, return_sequences=True, recurrent_initializer='glorot_uniform',stateful=True)  \n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "  def call(self, x):\n",
    "    embedding = self.embedding(x)\n",
    "    embedding = self.rnn(embedding)\n",
    "    prediction = self.fc(embedding)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jA8Pssrh1NKE"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "# Here, this is basically just a trick to avoid having \n",
    "# to one-hot encode the characters\n",
    "# I don't think it will add much otherwise\n",
    "# this would be more useful if we had a much larger vocabulary\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "units = 256\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrSRbSQk1Rcz"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(labels, logits):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-AILeTt71UXr",
    "outputId": "3a297655-d140-47b7-d3e4-7723d05dde0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      multiple                  4864      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  295680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  9766      \n",
      "=================================================================\n",
      "Total params: 310,310\n",
      "Trainable params: 310,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(tf.TensorShape([BATCH_SIZE, max_length]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjYtdulr8ukK"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# Checkpoint instance\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "GkLMi8GI1c9b",
    "outputId": "25fdf454-0acc-4a7c-c80f-edc14ac6a04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.6357\n",
      "Epoch 1 Batch 100 Loss 1.4112\n",
      "Epoch 1 Batch 200 Loss 1.1923\n",
      "Epoch 1 Loss 1.1918\n",
      "Time for epoch 98.48833298683167 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5047\n",
      "Epoch 2 Batch 100 Loss 1.1592\n",
      "Epoch 2 Batch 200 Loss 1.0597\n",
      "Epoch 2 Loss 1.1063\n",
      "Time for epoch 97.93208408355713 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2007\n",
      "Epoch 3 Batch 100 Loss 1.0772\n",
      "Epoch 3 Batch 200 Loss 1.0650\n",
      "Epoch 3 Loss 1.0276\n",
      "Time for epoch 98.16842603683472 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.0460\n",
      "Epoch 4 Batch 100 Loss 0.9909\n",
      "Epoch 4 Batch 200 Loss 0.9771\n",
      "Epoch 4 Loss 0.9318\n",
      "Time for epoch 98.10003304481506 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9893\n",
      "Epoch 5 Batch 100 Loss 0.9567\n",
      "Epoch 5 Batch 200 Loss 0.9387\n",
      "Epoch 5 Loss 0.9470\n",
      "Time for epoch 98.14890599250793 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.0364\n",
      "Epoch 6 Batch 100 Loss 0.9278\n",
      "Epoch 6 Batch 200 Loss 0.8705\n",
      "Epoch 6 Loss 0.9071\n",
      "Time for epoch 98.40014004707336 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9669\n",
      "Epoch 7 Batch 100 Loss 0.8884\n",
      "Epoch 7 Batch 200 Loss 0.8661\n",
      "Epoch 7 Loss 0.8792\n",
      "Time for epoch 99.25842905044556 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0075\n",
      "Epoch 8 Batch 100 Loss 0.8307\n",
      "Epoch 8 Batch 200 Loss 0.8685\n",
      "Epoch 8 Loss 0.8506\n",
      "Time for epoch 99.16148924827576 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.9140\n",
      "Epoch 9 Batch 100 Loss 0.8375\n",
      "Epoch 9 Batch 200 Loss 0.8124\n",
      "Epoch 9 Loss 0.8619\n",
      "Time for epoch 98.74792194366455 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9208\n",
      "Epoch 10 Batch 100 Loss 0.8437\n",
      "Epoch 10 Batch 200 Loss 0.7937\n",
      "Epoch 10 Loss 0.8234\n",
      "Time for epoch 98.34827089309692 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (input_seq, target_seq)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "             predictions = model(input_seq)\n",
    "            #print(predictions)\n",
    "             loss = loss_function(target_seq, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time for epoch {} sec\\n'.format(time.time() - start))\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0U_cmqGP8dwg",
    "outputId": "ca853395-b2e9-47a8-c69c-510e5b1ad4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  ckpt-4.data-00000-of-00001\r\n",
      "ckpt-1.data-00000-of-00001  ckpt-4.index\r\n",
      "ckpt-1.index                ckpt-5.data-00000-of-00001\r\n",
      "ckpt-10.data-00000-of-00001 ckpt-5.index\r\n",
      "ckpt-10.index               ckpt-6.data-00000-of-00001\r\n",
      "ckpt-11.data-00000-of-00001 ckpt-6.index\r\n",
      "ckpt-11.index               ckpt-7.data-00000-of-00001\r\n",
      "ckpt-12.data-00000-of-00001 ckpt-7.index\r\n",
      "ckpt-12.index               ckpt-8.data-00000-of-00001\r\n",
      "ckpt-2.data-00000-of-00001  ckpt-8.index\r\n",
      "ckpt-2.index                ckpt-9.data-00000-of-00001\r\n",
      "ckpt-3.data-00000-of-00001  ckpt-9.index\r\n",
      "ckpt-3.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K3VLTVT81rt"
   },
   "outputs": [],
   "source": [
    "# This is a hack to let us use the model with a different \n",
    "# batch size later\n",
    "model = Model(vocab_size, embedding_dim, units)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "Le_MRIFI1tRL",
    "outputId": "177ba82c-6f40-4f96-a592-25d89e856810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ken 160 103 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0efe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation step (generating text using the learned model)\n",
    "\n",
    "# Number of characters to generate\n",
    "num_generate = max_length\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = random.choice(string.ascii_lowercase)\n",
    "\n",
    "# Converting our start string to numbers (vectorizing) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# Empty string to store our results\n",
    "text_generated = []\n",
    "\n",
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "temperature = 0.5\n",
    "\n",
    "# Here batch size == 1\n",
    "model.reset_states()\n",
    "for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "    #print(predicted_id)\n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "generated_color = start_string + ''.join(text_generated).replace('<pad>', '')\n",
    "print(generated_color)\n",
    "\n",
    "try:\n",
    "  parts = generated_color.split()\n",
    "  r = float(parts[-3])\n",
    "  g = float(parts[-2])\n",
    "  b = float(parts[-1])\n",
    "  plt.clf()\n",
    "  _ = plt.imshow([[(r, g, b)]])\n",
    "  _ = plt.axis('off')\n",
    "  _ = plt.title(generated_color, fontsize=18)\n",
    "except:\n",
    "  print('unable to parse color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "8_colorbot_generate_starter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
